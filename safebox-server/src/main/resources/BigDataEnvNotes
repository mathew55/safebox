Notes on setting Big data environment in AWS
--------------------------------------------

Hadoop has 4 main component:
1) Distributed file system(way to store the data in different storage devices)
2) MapReduce (Map: takes input from different data sources as <key,value> pairs and produces another set of intermediate<key,value> pairs as output after some processes. Reduce: uses the output from the mappers and aggregates them for that key)
3) Hadoop Common(tools to read data from the hadoop file system)
4) YARN ( resource manager for data storage and analysis)


--> Decided to use CloudFormation Stack
	1) 3 EC2 instances: one master & two slaves
	2) 3 Elastic IP addresses and their EC2 instances.
	We don’t necessarily want our cluster to run permanently. However, by default, EC2 instances get different public IPs at every reboot, which can be annoying if you plan to bind the IPs to one of your domain (eg, to access easily your cluster’s web UI). Elastic IPs allow you to keep public IPs for a given EC2 instance’s life, with a small cost overhead.
	3)3 Security Groups
- hadoop-master ports configuration specific to Hadoop’s Master node
- hadoop-slave ports configuration specific to Hadoop’s Slave nodes
- ssh a simple rule to authorize SSH-ing on your instances
	Security Groups allow you to control your machines network access. You can see them individually as an OS-independant firewall rule, the advantage being that you only have to define the rules of a Security Group once, and then easily associate it with any machine within your EC2 pool.
